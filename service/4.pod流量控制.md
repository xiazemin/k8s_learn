我们肯定希望网络资源对于每个pod来说也是平均分配的，起码能做到每个pod的数据包发送与接收的速度都能控制在一定的范围内，节点的带宽不能被某个pod耗光而影响其它的pod，所以这一章我们来讨论如何控制pod的进出流量速率。

TC（traffic control）
linux自带流量控制框架，这个框架允许用户在数据发送前配置数据包排队规则qdisc（queueing discipline），对流量进行限制或整形，linux的tc只控制发送速率不控制接收速率，当然要控制接收速率也是有办法实现的。

对于限制网卡的发送速率，一般有两种方式：

第一种是对整个网卡的发送速率进行简单粗暴的限制，例如eth0网卡的发送速率限制100mbit；
第二种是对网卡发出的流量先进行分类，再分别对每一分类的的速率单独限制，例如访问mysql的流量限制80mbit，访问http服务的流量限制20mbit；
对于第一种方式，我们选择无类别队列，对于第二种方式，我们选择可分类队列。

无类别队列
主要有：

pfifo/bfifo（First In First Out）：先进先出队列，只需设置队列的长度，pfifo是以数据包的个数为单位，bfifo是以字节数为单位；
pfifo_fast：数据包是按照TOS被分配多三个波段里，band0优先级最高，band1次之，band2最低，按优先级从高到低发送，在每个波段里面，使用先进先出规则；
tbf（Token Bucket Filter）：针对数据字节数进行限制，适合于把流速降低到某个值，但允许短暂突发流量超过设定值，我们限制pod的发送速率主要就用这个队列；
red（Random Early Detection）：当带宽的占用接近于规定的带宽时，系统会随机地丢弃一些数据包
sfq（Stochastic Fairness Queueing）：它按照会话为流量进行排序，然后循环发送每个会话的数据包。
FQ (Fair Queue)：公平队列
在这里我们只介绍下如何创建tbf队列，因为后面主要是使用这个队列来限制pod的发送速率，下面的命令可以控制eth0的发送速率为100mbit：

tc qdisc add dev eth0 root tbf rate 100mbit burst 100mbit limit 100mbit

## rate：传输速率
## burst：桶的大小
## limit：确定最多有多少数据（byte）在队列中等待令牌
流量控制单位
- kbps：千字节／秒
- mbps：兆字节／秒
- kbit：KBits／秒
- mbit：MBits／秒
- bps：字节数／秒（如果不带单位，则默认单位是这个）
可分类队列
不同于无分类队列，可分类队列使用时步骤稍微复杂些，产要分三步：

给网卡创建一个队列
创建队列的分类，在各个分类里可以设置不同的流量策略
创建分类规则，把流量导到第二步创建的分类
可分类队列主要有：

cbq(class based queueing:基于类别排队)：没用过，自行google
htb(hierarchy token bucket:层级令牌桶)：看下面的示例
示例配置：访问mysql的速率限制80mbit，访问http服务的速率限制20mbit

创建htb队列：
tc qdisc add dev eth0 root handle 1: htb default 11 ##使用htb队列，默认流量去分类11
创建队列分类：
tc class add dev eth0 parent 1: classid 1:11 htb rate 80mbit ceil 80mbit  ## 创建分类11，速率限制80mbit
tc class add dev eth0 parent 1: classid 1:12 htb rate 20mbit ceil 20mbit  ## 创建分类12，速率限制20mbit
引导流量到分类：
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip dport 3306 0xffff flowid 1:11  ## 所有访问3306端口的流量导到分类11中
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip dport 80 0xffff flowid 1:12  ## 所有访问80端口的流量导到分类12中
也可以通过来源IP+目标端口来引导流量

tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 192.168.6.160 match ip dport 23 oxffff flowid 1:11
也可以通过数据标记来引导流量：

iptables -t mangle -A POSTROUTING -d 10.244.1.10 -j MARK –set-mark 100 ## 用iptables给数据打标记
tc filter add dev eth0 protocol ip parent 1:0 prio 2 handle 100 fw flowid 1:11  ## 标记了100的数据包引导到分类11中
pod的流量限制
然后在主机上安装iperf3：

yum install iperf3
因为pod使用的是veth网卡对，所以我们可以通过主机端的网卡，达到控制pod流量的目的；
tc qdisc add dev veth-pod-a root tbf rate 100mbit burst 100mbit limit 100mbit
删除接收限速：
tc qdisc del dev veth-pod-a root tbf rate 100mbit burst 100mbit limit 100mbit


控制pod的发包速率
因为linux的tc框架控发不控收，所以我们不能像上面一样，通过控制主机端veth-pod-a网卡的接收速率来控制pod发送速率的目的，但也不是完全没有办法：

在pod内直接限制发送速率
虽然不能在主机端控制pod的发送，但是可以直接在容器里控制，命令和上面的一样，只不过是在pod-a的ns中执行：

ip netns exec pod-a tc qdisc add dev eth0 root tbf rate 100mbit burst 100mbit limit 100mbit
因为是测试从容器发送的速率，所以我们要把iperf3的服务端调整一下，服务端跑在主机上，然后再在容器中进行发送测试：

ip netns exec pod-a iperf3 -c 192.168.6.160 -i 1 -t 10

在主机端用ifb网卡的方式限制收包速率
ifb网卡也是linux虚拟网络设备，类似于tun/tap/veth，只不过ifb的原理要简单得多，可以看作是一张只有tc过滤功能的虚拟网卡，而且它不会改变数据包的流向，比如把某张网卡接收流量导给ifb网卡，经过ifb的流量控制过滤后，继续走原网卡的接收流程，发送也是如此；这样我们就可以把pod在主机一端的网卡的接收重定向到ifb网卡，然后通过控制ifb网卡的发送速率，来间接控制pod的发送速率。

首先要确认内核有加载ifb模块，如果没有则加载

modprobe ifb    //需要加载ifb模块
然后创建ifb网卡，并设置发送队列长度为1000：

ip link add ifb0 type ifb
ip link set dev ifb0 up txqueuelen 1000
把veth-pod-a的接收重定向到ifb网卡上：

tc qdisc add dev veth-pod-a ingress handle ffff: 
tc filter add dev veth-pod-a parent ffff: protocol ip u32 match u32 0 0 flowid 1:1 action mirred egress redirect dev ifb0   //重定向流量到ifb
设置ifb0的发送速率：

tc qdisc add dev ifb0 root tbf rate 100mbit burst 100mbit limit 100mbit
此时再测：

ip netns exec pod-a  iperf3 -c 192.168.6.160 -i 1 -t 10

我们已经解决了一个常规则的k8s的cni需要解决的一切问题，但直到现在，也没见过go语言的影子，所以说k8s都是负责粘合功能的胶水代码，真正工作的是linux系统，与其说学习k8s的网络，不如说在学习linux提供的各种虚拟网络设备及内核协议栈的工作机制。